{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNEsWPlO+tdgkPMR7I4LLD9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["This notebook will will give details on how to evaluate nuclear segmentation models using REET2.0 on Google Colab. First, you must add the toolbox to your Google drive."],"metadata":{"id":"F0_L7JvNmMCy"}},{"cell_type":"markdown","source":["Mount your drive, ensure you are using a GPU. (Go to Runtime/Change Runtime Type/ and select a hardware accelerator)"],"metadata":{"id":"C5-BBooemmPE"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"iefGjMtlmM4q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Move to the parent directory. This will depend on where you saved your toolbox."],"metadata":{"id":"8YEX41FCnIOC"}},{"cell_type":"code","source":["cd /content/drive/MyDrive/Final_Toolbox\n"],"metadata":{"id":"kbyDJBPam8D2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will first demonstrate how to evaluate semantic segmentation models. We must first load the model. You can select one of the models that we trained for experiments from below."],"metadata":{"id":"mhLtS3tjnrgq"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","#U-Net RA\n","# model = torch.load('segmentation/output/U-Net_NA.pth')\n","\n","#U-Net NA\n","model = torch.load('segmentation/output/U-Net_RA.pth')\n","\n","#U-Net Pix\n","# model = torch.load('segmentation/output/U_Net_Pix.pth')\n"],"metadata":{"id":"SIYwLcLjncKW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the list of transforms you wish to evalutate"],"metadata":{"id":"4pausoFsqsSA"}},{"cell_type":"code","source":["transform_list = [\"Pixel\",\"Rotate\", \"Zoom Out\", \"HED Stain\", \"Mean\", \"Zoom In\", \"Crop\", \"Stain\", \"Blur\"]\n","#transform_list = [\"Pixel\"]"],"metadata":{"id":"xTmboQS7qWcd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The script below is the one we used to evaluate our U-Net.\n","\n","reetoolbox.evaluator.Semantic_Segmentation_Evaluator takes a model, a list of directories of test data, a transform, and the output shape of the model. This initalises the evaluator for a given transform. The transform parameters can be changed in reetoolbox/constants.py.\n","\n","\n","I have included a small sample of images in Consep_patches_540x540 to use for evaluation. The toolbox expects an np array for each image that includes the image, inst map and type map, in that order.\n","\n","We then use eval.predict to perform the evaluation. Adersarial should be set to true. To display the models performance on each transform, set display=True. The display can be buggy in Colab, if so, you can change the evalutator code to use plt.savefig instead of plt.show. Scale_perturbation will scale the pixel intensities to use the full pixel range, this is neeeded to see the Pixel perturbations.\n","\n","predict returns two dictionaries, one for the evaluation on the vanilla images and one for the evaluation on the transformed images. All metrics are calculated over the entire dataset by calculating TP, FP and FN for the entire dataset.\n","\n","Note: If you display the images, you may wish to restrict the number of images you are evalutating, as displaying will slow it down dramatically. I like to add a breakpoint to the display section of the predict function when I use display."],"metadata":{"id":"DOndjsL8rAMp"}},{"cell_type":"code","source":["import reetoolbox.evaluator\n","\n","for transform in transform_list:\n","    print(transform)\n","    eval = reetoolbox.evaluator.Semantic_Segmentation_Evaluator(model, [\"Consep_patches_540x540/valid/540x540_164x164\"], transform, evaluation_shape=(256,256))\n","    orig_results_dict, adv_results_dict = eval.predict(adversarial=True, display=False, scale_perturbation=True)\n","\n","    #SEMANTIC SEGMENTATION METRICS\n","    orig_ss_iou = orig_results_dict[\"pixel_wise_nucleus_IoU\"]\n","    adv_ss_iou = adv_results_dict[\"pixel_wise_nucleus_IoU\"]\n","\n","    orig_ss_dice =  orig_results_dict[\"pixel_wise_nucleus_Dice\"]\n","    adv_ss_dice = adv_results_dict[\"pixel_wise_nucleus_Dice\"]\n","\n","    orig_ss_acc =  orig_results_dict[\"all_type_pixel_accuracy\"]\n","    adv_ss_acc = adv_results_dict[\"all_type_pixel_accuracy\"]\n","\n","    orig_ss_type_IoU = orig_results_dict[\"pixel_wise_type_IoU\"]\n","    adv_ss_type_IoU = adv_results_dict[\"pixel_wise_type_IoU\"]\n","\n","    orig_ss_type_dice = orig_results_dict[\"pixel_wise_type_Dice\"]\n","    adv_ss_type_dice = adv_results_dict[\"pixel_wise_type_Dice\"]\n","\n","\n","\n","    print(\"SEMANTIC SEGMENTATION METRICS\")\n","    print(f\"original pixel_wise nucleus pixel IoU: {orig_ss_iou}\")\n","    print(f\"adversarial pixel_wise nucleus IoU: {adv_ss_iou}\")\n","    print(\"\")\n","    print(f\"original pixel_wise nucleus pixel dice {orig_ss_dice}\")\n","    print(f\"adversarial pixel_wise dice {adv_ss_dice}\")\n","    print(\"\")\n","    print(f\"original pixel_wise classification Iou for each type: {orig_ss_type_IoU}\")\n","    print(f\"adversarial Iou for each type: {adv_ss_type_IoU}\")\n","    print(\"\")\n","    print(f\"original pixel_wise classification dice for each type: {orig_ss_type_dice}\")\n","    print(f\"adversarial pixel_wise dice for each type: {adv_ss_type_dice}\")\n","\n","    #THE MEAN TYPE_DICE IS THE ONE WE USED FOR OUR EXPERIMENTS\n","    mean_dice = (adv_ss_type_dice[2] + adv_ss_type_dice[3] + adv_ss_type_dice[4])/3\n","    print(f\"mean type dice: {mean_dice}\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")"],"metadata":{"id":"TM9XLu1JqySw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To evaluate HoVer-Net, we first need to load a model. Note that the way you load your weights may differ if you did not use parallelisation during your model training.\n","\n","\n"],"metadata":{"id":"0qBUMQtQubJm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpPpmcz4-gK8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","#HoVer-Net NA\n","# weights = torch.load(\"nuc_inst_segmentation/output/HoVer-Net_NA.tar\")[\"desc\"]\n","\n","#HoVer-Net RA\n","weights = torch.load(\"nuc_inst_segmentation/output/HoVer-Net_RA.tar\")[\"desc\"]\n","\n","#HoVer-Net Pix\n","# weights = torch.load(\"nuc_inst_segmentation/output/HoVer-Net_Pix.tar\")[\"desc\"]\n","\n","\n","#LOAD THE WEIGHTS INTO THE MODEL\n","from nuc_inst_segmentation.hovernet.net_desc import create_model\n","\n","net_desc = create_model(input_ch=3, \n","              nr_types=5, \n","              freeze=False,\n","              mode=\"original\").to(\"cuda\")\n","\n","from nuc_inst_segmentation.hovernet.utils import convert_pytorch_checkpoint\n","\n","weights = convert_pytorch_checkpoint(weights)\n","\n","load_feedback = net_desc.load_state_dict(weights, strict=False)\n"]},{"cell_type":"markdown","source":["Now we can evaluate the HoVer-Net model. The HoVer_evaluator and predict work analagously to the Semantic_Segmentation_Evaluator."],"metadata":{"id":"m_6A-v-I6s5j"}},{"cell_type":"code","source":["import reetoolbox.evaluator\n","\n","for transform in transform_list:\n","    print(transform)\n","    eval = reetoolbox.evaluator.HoVer_Evaluator(net_desc, [\"Consep_patches_540x540/valid/540x540_164x164\"], transform)\n","    orig_results_dict, adv_results_dict = eval.predict(adversarial=True, display=False, scale_perturbation=True)\n","\n","\n","    #SEMANTIC SEGMENTATION METRICS\n","    orig_ss_iou = orig_results_dict[\"pixel_wise_nucleus_IoU\"]\n","    adv_ss_iou = adv_results_dict[\"pixel_wise_nucleus_IoU\"]\n","\n","    orig_ss_dice =  orig_results_dict[\"pixel_wise_nucleus_Dice\"]\n","    adv_ss_dice = adv_results_dict[\"pixel_wise_nucleus_Dice\"]\n","\n","    orig_ss_acc =  orig_results_dict[\"all_type_pixel_accuracy\"]\n","    adv_ss_acc = adv_results_dict[\"all_type_pixel_accuracy\"]\n","\n","    orig_ss_type_IoU = orig_results_dict[\"pixel_wise_type_IoU\"]\n","    adv_ss_type_IoU = adv_results_dict[\"pixel_wise_type_IoU\"]\n","\n","    orig_ss_type_dice = orig_results_dict[\"pixel_wise_type_Dice\"]\n","    adv_ss_type_dice = adv_results_dict[\"pixel_wise_type_Dice\"]\n","\n","    mean_dice = (adv_ss_type_dice[2] + adv_ss_type_dice[3] + adv_ss_type_dice[4])/3\n","    print(f\"mean type dice: {mean_dice}\")\n","\n","    #NUCLEUS INSTANCE SEGMENTATION METRICS\n","    orig_inst_dice = orig_results_dict[\"instance_wise_Dice\"]\n","    adv_inst_dice = adv_results_dict[\"instance_wise_Dice\"]\n","\n","    orig_inst_sq = orig_results_dict[\"instance_wise_segmentation_quality\"]\n","    adv_inst_sq = adv_results_dict[\"instance_wise_segmentation_quality\"]\n","\n","    orig_inst_pq = orig_results_dict[\"instance_wise_panoptic_quality\"]\n","    adv_inst_pq = adv_results_dict[\"instance_wise_panoptic_quality\"]\n","\n","    #NUCLEUS INSTANCE TYPE CLASSIFICATION METRICS\n","    orig_inst_type_accuracy = orig_results_dict[\"instance_wise_type_classification_accuracy\"]\n","    adv_inst_type_accuracy = adv_results_dict[\"instance_wise_type_classification_accuracy\"]\n","\n","\n","    print(\"SEMANTIC SEGMENTATION METRICS\")\n","    print(f\"original pixel_wise nucleus pixel IoU: {orig_ss_iou}\")\n","    print(f\"adversarial pixel_wise nucleus IoU: {adv_ss_iou}\")\n","    print(\"\")\n","    print(f\"original pixel_wise nucleus pixel dice {orig_ss_dice}\")\n","    print(f\"adversarial pixel_wise dice {adv_ss_dice}\")\n","    print(\"\")\n","    print(f\"original pixel_wise accuracy {orig_ss_acc}\")\n","    print(f\"adversarial pixel_wise accuracy {adv_ss_acc}\")\n","    print(\"\")\n","    print(f\"original pixel_wise classification Iou for each type: {orig_ss_type_IoU}\")\n","    print(f\"adversarial Iou for each type: {adv_ss_type_IoU}\")\n","    print(\"\")\n","    print(f\"original pixel_wise classification dice for each type: {orig_ss_type_dice}\")\n","    print(f\"adversarial pixel_wise dice for each type: {adv_ss_type_dice}\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"NUCLEUS INSTANCE SEGMENTATION METRICS\")\n","    print(f\"original instance_wise dice: {orig_inst_dice}\")\n","    print(f\"adversarial instance_wise dice: {adv_inst_dice}\")\n","    print(\"\")\n","    print(f\"original instance_wise segmentation quality: {orig_inst_sq}\")\n","    print(f\"adversarial instance_wise segmentation quality: {adv_inst_sq}\")\n","    print(\"\")\n","    print(f\"original instance_wise panoptic quality: {orig_inst_pq}\")\n","    print(f\"adversarial instance_wise panoptic quality: {adv_inst_pq}\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"NUCLEUS INSTANCE TYPE CLASSIFICATION\")\n","    print(f\"original instance_wise type classification accuracy: {orig_inst_type_accuracy}\")\n","    print(f\"adversarial instance_wise type classification accuracy {adv_inst_type_accuracy}\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")\n","    print(\"\")"],"metadata":{"id":"_tI_9RF26pfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YBS2hVm7HeVC"},"execution_count":null,"outputs":[]}]}