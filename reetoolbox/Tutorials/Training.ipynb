{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training.ipynb","provenance":[],"authorship_tag":"ABX9TyPlJ0pJK4p5aRiTKGOp3fVo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Training Robust Models\n","\n","This notebook will demonstrate how you can include adversarial transforms during training to train more robust and potentially more accurate models.\n","\n","The basic process is very simple - we create our desired training loop as normal, and then add an adversarial transform to transform a batch of data before we train on it. This is very similar to standard data augmentation, but the adversarial optimisation finds more challenging transformations, resulting in greater improvements in robustness."],"metadata":{"id":"vYfMJP-5v6ro"}},{"cell_type":"markdown","source":["Install the toolbox"],"metadata":{"id":"D87lTjXxe-6k"}},{"cell_type":"code","source":["!pip install reetoolbox"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2D8kaQie8sy","executionInfo":{"status":"ok","timestamp":1642432504234,"user_tz":0,"elapsed":7816,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}},"outputId":"7b6bbded-24ac-46c7-95ec-c3febe1b56d3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting reetoolbox\n","  Downloading reetoolbox-0.1.0.tar.gz (13 kB)\n","Requirement already satisfied: numpy==1.19.5 in /usr/local/lib/python3.7/dist-packages (from reetoolbox) (1.19.5)\n","Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.7/dist-packages (from reetoolbox) (1.0.2)\n","Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from reetoolbox) (1.10.0+cu111)\n","Collecting torchvision==0.2.1\n","  Downloading torchvision-0.2.1-py2.py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from reetoolbox) (3.2.2)\n","Requirement already satisfied: seaborn==0.11.2 in /usr/local/lib/python3.7/dist-packages (from reetoolbox) (0.11.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from reetoolbox) (7.1.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.2->reetoolbox) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.2->reetoolbox) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.2->reetoolbox) (1.1.0)\n","Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn==0.11.2->reetoolbox) (1.1.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->reetoolbox) (3.10.0.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.2.1->reetoolbox) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->reetoolbox) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->reetoolbox) (3.0.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->reetoolbox) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->reetoolbox) (1.3.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn==0.11.2->reetoolbox) (2018.9)\n","Building wheels for collected packages: reetoolbox\n","  Building wheel for reetoolbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for reetoolbox: filename=reetoolbox-0.1.0-py3-none-any.whl size=15761 sha256=4d31c56adbda9931acb7bd02800e037ee869a4602120f4c5572c0a7aadacd22c\n","  Stored in directory: /root/.cache/pip/wheels/7b/67/32/58fb18b077c661c8037f07368b00421439401f770d64b2ad81\n","Successfully built reetoolbox\n","Installing collected packages: torchvision, reetoolbox\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.11.1+cu111\n","    Uninstalling torchvision-0.11.1+cu111:\n","      Successfully uninstalled torchvision-0.11.1+cu111\n","Successfully installed reetoolbox-0.1.0 torchvision-0.2.1\n"]}]},{"cell_type":"markdown","source":["If you're using Google Colab, you need to mount your drive using the cell below. Otherwise you can skip this."],"metadata":{"id":"tnAdcTPLfnSK"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5A8j8I11fd17","executionInfo":{"status":"ok","timestamp":1642432531838,"user_tz":0,"elapsed":22017,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}},"outputId":"fb2f717a-b815-43d8-9423-856a373e52a3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["You must set the variable PATH to the directory containing this file. It will probably look like the below path if you cloned the https://github.com/alexjfoote/reetoolbox-tutorials repo to your Google Drive."],"metadata":{"id":"rPdr9VY2gOO6"}},{"cell_type":"code","source":["PATH = \"/content/drive/My Drive/reetoolbox-tutorials\""],"metadata":{"id":"mW1qJ662f7_E","executionInfo":{"status":"ok","timestamp":1642432531840,"user_tz":0,"elapsed":13,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Import some useful functions"],"metadata":{"id":"-9eS-aOUgzue"}},{"cell_type":"code","source":["from reetoolbox.utils import load_resnet, load_pannuke, get_dataloader"],"metadata":{"id":"eZzLHgi6gyJy","executionInfo":{"status":"ok","timestamp":1642432646156,"user_tz":0,"elapsed":276,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Define the device we're using and the class names"],"metadata":{"id":"GCjXy6qtliub"}},{"cell_type":"code","source":["import torch\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","classes = [\"Negative\", \"Positive\"]"],"metadata":{"id":"_Ge-m5a_lf46","executionInfo":{"status":"ok","timestamp":1642432650284,"user_tz":0,"elapsed":256,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Load the PanNuke dataset (see https://jgamper.github.io/PanNukeDataset/,  J. Gamper, N. A. Koohbanani, K. Benet, A. Khuram, and N. Rajpoot,\n","“PanNuke: An Open Pan-Cancer Histology Dataset for Nuclei Instance\n","Segmentation and Classification,” in Digital Pathology, pp. 11–19,\n","Springer, Cham, Apr. 2019.)"],"metadata":{"id":"mKUNP8gYhNi0"}},{"cell_type":"code","source":["import os\n","\n","data_path = os.path.join(PATH, \"Data/breast_folds.npz\")\n","Xtr, ytr, Xts, yts = load_pannuke(data_path)"],"metadata":{"id":"PkYwv15Xgvkw","executionInfo":{"status":"ok","timestamp":1642432726019,"user_tz":0,"elapsed":22632,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["Next we split our training data into a train and validation set and create a dictionary containing a training and validation data loader. We also create a dataloader from our test set."],"metadata":{"id":"zv7GKotFxCMV"}},{"cell_type":"code","source":["# Create dataloaders for the training loop\n","batch_size = 16\n","val_batch_size = batch_size\n","\n","# Number of epochs to train for\n","num_epochs = 15\n","\n","length = int(0.7 * len(Xtr))\n","\n","train_Xtr = Xtr[:length]\n","val_Xtr = Xtr[length:]\n","train_ytr = ytr[:length]\n","val_ytr = ytr[length:]\n","\n","train_data = torch.utils.data.TensorDataset(train_Xtr, train_ytr)\n","val_data = torch.utils.data.TensorDataset(val_Xtr, val_ytr)\n","\n","test_data = torch.utils.data.TensorDataset(Xts, yts)\n","test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size, shuffle = False)\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","num_workers = 2\n","\n","dataloaders_dict = {\n","    \"train\": torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers),\n","    \"val\": torch.utils.data.DataLoader(val_data, batch_size=val_batch_size, shuffle=True, num_workers=num_workers)\n","}"],"metadata":{"id":"BFY_nrUFxBJ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642432730117,"user_tz":0,"elapsed":271,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}},"outputId":"b7947ef8-f85f-497a-e96a-3217226f7697"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing Datasets and Dataloaders...\n"]}]},{"cell_type":"markdown","source":["We load a pretrained model that we will fine-tune on our dataset and replace the final layer to fit our application (2 output neurons). We then gather all the parameters to update and create an optimiser for use during training."],"metadata":{"id":"p6PnaPnFzgig"}},{"cell_type":"code","source":["from torchvision import models\n","from torch import optim\n","\n","def load_model(n_classes=2):\n","    model = models.resnet18(pretrained=True)  \n","    model.fc = nn.Sequential(nn.Linear(512, n_classes),        \n","                            nn.LogSoftmax(dim=1))\n","    model = model.to(device)\n","    model.train()  \n","\n","    params_to_update = model.parameters()\n","    optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n","    return model, optimizer_ft"],"metadata":{"id":"OylwrJgkzV9k","executionInfo":{"status":"ok","timestamp":1642432893738,"user_tz":0,"elapsed":12,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["Create a standard training loop, which also takes a function that takes a batch of inputs and returns a transformed batch, which we can use to wrap our adversarial transforms."],"metadata":{"id":"Sflabwo6423q"}},{"cell_type":"code","source":["import copy\n","import time\n","\n","def train_loop(model, dataloaders, criterion, optimizer, epochs, device=\"cuda:0\", transform_func=None, **kwargs):\n","    acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    t_full = time.time()\n","\n","    train_loader = dataloaders[\"train\"]\n","    val_loader = dataloaders[\"val\"]\n","\n","    for epoch in range(epochs): \n","        print(f'Epoch {epoch + 1}/{epochs}')\n","        print('-' * 10)   \n","        t_epoch = time.time() \n","\n","        num_examples = 0\n","        running_loss = 0.0\n","        running_corrects = 0\n","\n","        model.train()\n","\n","        for stage in dataloaders:            \n","            for batch_no, (inputs, labels) in enumerate(train_loader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)  \n","\n","                with torch.set_grad_enabled(True):\n","                    optimizer.zero_grad()    \n","\n","                    if stage == \"train\":\n","                        model.train()\n","\n","                        if transform_func is not None:\n","                            inputs = transform_func(model, inputs, labels, **kwargs)  \n","\n","                        outputs = model(inputs)  \n","                        _, preds = torch.max(outputs, 1)\n","\n","                        loss = criterion(outputs, labels)\n","                                            \n","                        loss.backward()\n","                        optimizer.step()\n","                    else:\n","                        model.eval()\n","                        outputs = model(inputs)  \n","                        _, preds = torch.max(outputs, 1)\n","\n","                    num_examples += len(preds)\n","                    running_corrects += torch.sum(preds == labels.data)\n","                    running_loss += loss.item()\n","\n","            epoch_loss = running_loss / len(train_loader)\n","            epoch_acc = running_corrects.double() / num_examples\n","            print(f'{stage}: Loss: {round(epoch_loss, 3)} Acc: {round(epoch_acc.item(), 3)}')\n","\n","            if stage == \"val\" and epoch_acc >= best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print(f\"{round(time.time() - t_epoch, 2)}s\")  \n","\n","        acc_history.append(epoch_acc)\n","\n","    print(f\"Took: {round(time.time() - t_full, 2)}s\")\n","    print(f'Best val Acc: {best_acc}')\n","    model.load_state_dict(best_model_wts)\n","    return model, np.array(acc_history)"],"metadata":{"id":"o4L7fIXCznl6","executionInfo":{"status":"ok","timestamp":1642433421000,"user_tz":0,"elapsed":245,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["Next we import the desired transforms and corresponding optimisers and parameters, as well as the evaluator, and create a function that will measure the accuracy and robustness of a model to the stain transform."],"metadata":{"id":"ifCWQsKv_ri4"}},{"cell_type":"code","source":["from reetoolbox.transforms import StainTransform\n","from reetoolbox.optimisers import PGD\n","from reetoolbox.constants import (eval_stain_transform_params,\n","                            eval_stain_optimiser_params)\n","from reetoolbox.evaluator import Evaluator\n","from reetoolbox.metrics import get_metrics\n","\n","def evaluate(model, test_dataset, test_dataloader):\n","    stain_evaluator = Evaluator(model, test_dataset, test_dataloader, PGD, \n","                                StainTransform, eval_stain_optimiser_params, \n","                                eval_stain_transform_params, device=device)\n","    results = stain_evaluator.predict(adversarial=True)\n","    get_metrics(results)"],"metadata":{"id":"fEI1lDct5qft","executionInfo":{"status":"ok","timestamp":1642433664307,"user_tz":0,"elapsed":643,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":["To start we train a model without any adversarial transforms to get a baseline."],"metadata":{"id":"ugCAU3CMABmF"}},{"cell_type":"code","source":["from torch import nn\n","\n","std_model, optimizer_ft = load_model(n_classes=2)\n","std_model, _ = train_loop(std_model, dataloaders_dict, nn.CrossEntropyLoss(), optimizer_ft, \n","           10, device=device, transform_func=None)\n","evaluate(std_model, test_data, test_loader)"],"metadata":{"id":"B9F4_y7a4yz8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642433723445,"user_tz":0,"elapsed":56153,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}},"outputId":"20c1f7f1-eddb-4940-d521-3bf2d42f53f3"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","----------\n","train: Loss: 0.517 Acc: 0.733\n","val: Loss: 0.675 Acc: 0.835\n","3.79s\n","Epoch 2/10\n","----------\n","train: Loss: 0.205 Acc: 0.918\n","val: Loss: 0.514 Acc: 0.918\n","3.74s\n","Epoch 3/10\n","----------\n","train: Loss: 0.073 Acc: 0.988\n","val: Loss: 0.268 Acc: 0.992\n","3.75s\n","Epoch 4/10\n","----------\n","train: Loss: 0.047 Acc: 0.989\n","val: Loss: 0.286 Acc: 0.98\n","3.73s\n","Epoch 5/10\n","----------\n","train: Loss: 0.045 Acc: 0.987\n","val: Loss: 0.271 Acc: 0.993\n","3.77s\n","Epoch 6/10\n","----------\n","train: Loss: 0.054 Acc: 0.986\n","val: Loss: 1.076 Acc: 0.949\n","3.76s\n","Epoch 7/10\n","----------\n","train: Loss: 0.038 Acc: 0.989\n","val: Loss: 0.053 Acc: 0.995\n","3.77s\n","Epoch 8/10\n","----------\n","train: Loss: 0.03 Acc: 0.991\n","val: Loss: 0.036 Acc: 0.996\n","3.77s\n","Epoch 9/10\n","----------\n","train: Loss: 0.013 Acc: 0.999\n","val: Loss: 0.078 Acc: 0.999\n","3.78s\n","Epoch 10/10\n","----------\n","train: Loss: 0.016 Acc: 0.997\n","val: Loss: 0.094 Acc: 0.998\n","3.76s\n","Took: 37.62s\n","Best val Acc: 0.999455930359086\n","Accuracy: 0.928, robust accuracy: 0.268, fooling ratio: 0.712\n"]}]},{"cell_type":"markdown","source":["Next we import `apply_transforms` and the default parameters for creating a stain adversarial optimiser. \n","\n","We create a list of tuples containing the optimisers and their adversarial optimiser parameters. We then iterate over this list and use the parameters to set up each optimiser. \n","\n","We can give this list of adversarial optimisers to `apply_transforms` and it will sample `k` optimisers and sequentially transform the data using the sampled adversarial optimisers. \n","\n","We pass `apply_transforms`, `k`, and the list of adversarial optimisers to the training loop, which will use them to transform each batch of data."],"metadata":{"id":"VMetTTzTAIia"}},{"cell_type":"code","source":["from reetoolbox.trainer import apply_transforms\n","from reetoolbox.constants import stain_adv_opt_params\n","\n","adv_model, optimizer_ft = load_model(n_classes=2)\n","\n","optimizers_and_params = [(PGD, stain_adv_opt_params)]\n","\n","all_adv_opts = []\n","for TransformOptimiser, adv_opt_params in optimizers_and_params:\n","    all_adv_opts.append(TransformOptimiser(adv_model, **adv_opt_params, device=device))\n","\n","adv_model, _ = train_loop(adv_model, dataloaders_dict, nn.CrossEntropyLoss(), optimizer_ft, \n","           epochs=10, device=device, transform_func=apply_transforms, k=1, \n","           adv_optimisers=all_adv_opts)\n","\n","evaluate(adv_model, test_data, test_loader)"],"metadata":{"id":"HIHi31vD6n_f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642433888177,"user_tz":0,"elapsed":112478,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}},"outputId":"eedb2d7d-7a88-4ee2-d78c-e3bb1cefb95c"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","----------\n","train: Loss: 0.687 Acc: 0.606\n","val: Loss: 1.769 Acc: 0.739\n","9.41s\n","Epoch 2/10\n","----------\n","train: Loss: 0.489 Acc: 0.764\n","val: Loss: 0.872 Acc: 0.847\n","9.32s\n","Epoch 3/10\n","----------\n","train: Loss: 0.424 Acc: 0.81\n","val: Loss: 0.842 Acc: 0.86\n","9.34s\n","Epoch 4/10\n","----------\n","train: Loss: 0.331 Acc: 0.857\n","val: Loss: 0.49 Acc: 0.904\n","9.33s\n","Epoch 5/10\n","----------\n","train: Loss: 0.237 Acc: 0.911\n","val: Loss: 0.888 Acc: 0.943\n","9.36s\n","Epoch 6/10\n","----------\n","train: Loss: 0.215 Acc: 0.924\n","val: Loss: 1.177 Acc: 0.958\n","9.34s\n","Epoch 7/10\n","----------\n","train: Loss: 0.192 Acc: 0.923\n","val: Loss: 0.428 Acc: 0.958\n","9.4s\n","Epoch 8/10\n","----------\n","train: Loss: 0.117 Acc: 0.954\n","val: Loss: 0.591 Acc: 0.977\n","9.34s\n","Epoch 9/10\n","----------\n","train: Loss: 0.128 Acc: 0.946\n","val: Loss: 0.195 Acc: 0.971\n","9.34s\n","Epoch 10/10\n","----------\n","train: Loss: 0.073 Acc: 0.982\n","val: Loss: 0.268 Acc: 0.991\n","9.37s\n","Took: 93.55s\n","Best val Acc: 0.9907508161044614\n","Accuracy: 0.921, robust accuracy: 0.890, fooling ratio: 0.033\n"]}]},{"cell_type":"markdown","source":["As you can see, including the adversarial stain transform improved the model's accuracy, and significantly improved it's robustness to the stain transform, compared to the model trained without the transform."],"metadata":{"id":"77EVYfEVBpq_"}},{"cell_type":"markdown","source":["You can also use the built in implementation of adversarial training for free (see https://arxiv.org/abs/1904.12843), adapted for use with multiple adversarial transforms. This reorders the data, repeating each `m` times in a row and performing the optimisation over the repetitions for more efficient adversarial training. This is beneficial when you want to a more expensive adversarial optimisation, such as multi-step PGD - you can do one step per repetition of the batch and reach comparable robustness with lower training times.\n","\n","You can also use the built in evaluation function, which will take a list of dictionaries that contain all the info needed to set up and run an evaluation, and perform all the evaluations."],"metadata":{"id":"lj_erLBXs0T6"}},{"cell_type":"code","source":["from reetoolbox.trainer import train, evaluation\n","\n","model_params = {\n","    \"path\": None,\n","    \"load_saved\": False,\n","    \"pretrained\": True,\n","    \"n_classes\": 2\n","}\n","\n","train_params = {\n","    \"dataloaders\": dataloaders_dict,\n","    \"criterion\": nn.CrossEntropyLoss(),\n","    \"initial_epochs\": 0,\n","    \"adv_epochs\": 10,\n","    \"m\": 1,\n","    \"last_n\": 10,\n","    \"k\": 1\n","}\n","\n","adv_free_model, optimizer_ft = load_model()\n","adv_free_model = train(adv_free_model, optimizer_ft, optimizers_and_params, \n","                          train_params, device=device)\n","\n","stain_evaluator_params = {\n","    \"dataset\": test_data,\n","    \"dataloader\": test_loader,\n","    \"TransformOptimiser\": PGD,\n","    \"Transform\": StainTransform,\n","    \"optimiser_params\": eval_stain_optimiser_params,\n","    \"trans_params\": eval_stain_transform_params\n","}\n","\n","evaluator_params = [stain_evaluator_params]\n","\n","evaluation(adv_free_model, evaluator_params, device=device)"],"metadata":{"id":"uLv6ZE0jrwt-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642434633896,"user_tz":0,"elapsed":106592,"user":{"displayName":"Alex Foote","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09524383492218707220"}},"outputId":"9cefa354-1eae-4915-a3e3-9e13b0ec26a5"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Epochs... \n","Epoch 1/0\n","----------\n","Took: 0.0s\n","Best val Acc: 0.0\n","Adversarial Epochs...\n","Epoch 1/10\n","----------\n","Train: Loss: 0.717 Acc: 0.578\n","Val: Loss: 0.498 Acc: 0.777\n","8.6s\n","Epoch 2/10\n","----------\n","Train: Loss: 0.535 Acc: 0.754\n","Val: Loss: 0.455 Acc: 0.787\n","8.74s\n","Epoch 3/10\n","----------\n","Train: Loss: 0.385 Acc: 0.827\n","Val: Loss: 0.332 Acc: 0.838\n","8.7s\n","Epoch 4/10\n","----------\n","Train: Loss: 0.266 Acc: 0.881\n","Val: Loss: 0.376 Acc: 0.851\n","8.73s\n","Epoch 5/10\n","----------\n","Train: Loss: 0.243 Acc: 0.904\n","Val: Loss: 0.398 Acc: 0.835\n","8.73s\n","Epoch 6/10\n","----------\n","Train: Loss: 0.186 Acc: 0.928\n","Val: Loss: 0.437 Acc: 0.825\n","8.7s\n","Epoch 7/10\n","----------\n","Train: Loss: 0.14 Acc: 0.945\n","Val: Loss: 0.334 Acc: 0.873\n","8.69s\n","Epoch 8/10\n","----------\n","Train: Loss: 0.119 Acc: 0.952\n","Val: Loss: 0.425 Acc: 0.846\n","8.71s\n","Epoch 9/10\n","----------\n","Train: Loss: 0.106 Acc: 0.961\n","Val: Loss: 0.456 Acc: 0.835\n","8.73s\n","Epoch 10/10\n","----------\n","Train: Loss: 0.076 Acc: 0.976\n","Val: Loss: 0.476 Acc: 0.853\n","8.72s\n","Took: 87.19s\n","Best val Acc: 0.8734177215189873\n","<class 'reetoolbox.transforms.StainTransform'>\n","Accuracy: 0.908, robust accuracy: 0.864, fooling ratio: 0.049\n"]}]}]}